<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project
  | ECE, Virginia Tech | Fall 2020: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
table, th, td {
  border: 1px solid black;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Heart Rate Detection Using Remote Photoplethysmography</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>David Hass, Spencer Mullinix, Hogan Pope</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2020 ECE 4554/5554 Computer Vision: Course Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
<hr>

<!-- Goal -->
<h3>Abstract</h3>
Currently heart rate is an attribute that can be incredibly difficult to measure without being in close proximity to the patient. However, by using modern Computer Vision techniques, a close approximation to heart rate can be discovered with nothing more than a live video feed.
<br><br>
<!-- figure -->
<h3>Teaser figure</h3>
An example of what we hope the final project to look like.
<br><br>
<!-- Main Illustrative Figure --> 
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="face.png">
</div>

<br><br>
<!-- Introduction -->
<h3>Introduction</h3>
    Finding a client's heart rate either for health, polygraph, or other reasons, is an issue that classically requires close proximity. However, with recent developments in remote photoplethysmography, this is no longer necessarily the case. If a stable version of this can be created and easily deployed, it would allow for better remote healthcare work. As well as improvements in other areas where being remote can help lower costs or increase availability.
This project will be done entirely in the RBG domain, as part of the aim is to make these tools possible to as wide an array of people as possible. Hopefully being able to make them deployable on nearly all modern laptops, as well as potentially smartphones.
One of the ways this issue has been approached in the past, specifically in the realm of smart phones, is through the use of fingerprint scanners. However, one of the benefits of being able to do this entirely via camera, is that while fingerprint scanners are becoming increasingly common in smartphones, they are all but non-existent in laptops, and many other devices that already have integrated cameras. Thus using only a camera would increase the domain of devices that could be supported.


<br><br>
<!-- Approach -->
<h3>Approach</h3>
We plan to implement an image processing pipeline aimed towards extracting a subject's blood volume pulse (BVP) signal, and from that, their pulse rate with a technique called remote photoplethysmography (rPPG). The algorithms will be fed a video of a subject, and process each frame of the video to extract BVP data. There are many ways to achieve this goal; we will experiment with two approaches in parallel. The first rPPG algorithm we’ll implement involves statistical and spectral analysis, and the second one involves using deep learning methods. We hope that through implementing both, we'll be able to see which techniques are most effective and combine useful aspects from each.<br><br>
 
The spectral method we will implement is inspired by Poe et al. [1] and consists of roughly 3 portions: ROI detection, preprocessing and extraction, and pulse rate calculation. The first of which is aimed to calculate the location of the subject's face to measure the BVP signal. We used the Open Computer Vision (opencv) library to utilize Haar Cascades to accomplish this. After their face is segmented, each RGB channel in the face-image will be averaged and normalized, resulting in one measurement per channel per frame in the video. These measurements will be gathered into three vectors and decomposed into three independent source signals using independent component analysis (ICA). These signals represent the fluctuations in color caused by variations in blood volume, and the one with the strongest power spectrum peak will be further analyzed. We  then filter the signal further in the time and frequency domains with a 5 point moving-average and then apply a hamming window bandpass filter with cuttoffs from 0.7 to 4 Hz. Since the sampling rate of the video varies from clip to clip, we used a cubic spline interpolation to fill out the values before applying the bandpass filter. The bandpass filter and the cubic spline interpolation were implemented with the help of the Scipy library. After filtering our data, we can calculate the interbeat intervals, and thus the heart rate.<br><br>

The second approach we will take is to implement a convolutional neural network to estimate a PPG signal. The model will use the difference between two frames of face-images as input, and attempt to estimate the PPG signal at that instance. The face image will be generated through segmentation using Haar Cascades, just as in our first approach. While the precise architecture has not been determined, similar studies have used upwards of 10 convolutional layers and 5 pooling layers [2].

<!--Source of this table http://www.es.ele.tue.nl/~sander/publications/tbme16-algorithmic-rppg.pdf-->
<br><br>
<!-- Results -->
<h3>Experiments</h3>
    We will be able to compare our methods to other popular methods of RPPG(Remote Photoplethysmography) as seen in the table below.<br>
    In this table, the values listed in each cell is the Signal-to-Noise-Ratio (SNR).<br>
    <table>
        <tr>
            <th>Category</th>
            <th>Challenge</th>
            <th>G(2007)</th>
            <th>G(2008)</th>
            <th>PCA(2011)</th>
            <th>ICA(2011)</th>
            <th>CHROM(2013)</th>
            <th>PBV(2014)</th>
            <th>2SR(2014)</th>
            <th>Spectral Method</th>
            <th>Nueral Network</th>
        </tr>
        <tr>
            <td rowspan="3">Skin Type</td>
            <td>Type I-II</td>
            <td>2.67</td>
            <td>7.55</td>
            <td>5.85</td>
            <td>6.51</td>
            <td>6.47</td>
            <td>5.57</td>
            <td>7.44</td>
            <td>TBD</td>
            <td>TBD</td>
        </tr>
        <tr>
            
            <td>Type III</td>
            <td>2.07</td>
            <td>7.89</td>
            <td>5.38</td>
            <td>6.61</td>
            <td>6.21</td>
            <td>6.26</td>
            <td>7.90</td>
            <td>TBD</td>
            <td>TBD</td>
        </tr>
        <tr>
            <td>Type IV-V</td>
            <td>-0.49</td>
            <td>6.40</td>
            <td>2.25</td>
            <td>4.56</td>
            <td>5.43</td>
            <td>4.04</td>
            <td>6.60</td>
            <td>TBD</td>
            <td>TBD</td>
        </tr>
        <tr>
            <td rowspan="3">Luminance</td>
            <td>Stationary</td>
            <td>8.10</td>
            <td>10.14</td>
            <td>8.70</td>
            <td>11.61</td>
            <td>9.42</td>
            <td>6.57</td>
            <td>10.53</td>
            <td>TBD</td>
            <td>TBD</td>
        </tr>
        <tr>
            <td>Rotation</td>
            <td>0.81</td>
            <td>3.34</td>
            <td>1.46</td>
            <td>4.04</td>
            <td>3.63</td>
            <td>6.36</td>
            <td>6.16</td>
            <td>TBD</td>
            <td>TBD</td>
        </tr>
        <tr>
            <td>Talking</td>
            <td>-0.62</td>
            <td>3.75</td>
            <td>0.46</td>
            <td>3.11</td>
            <td>3.99</td>
            <td>4.01</td>
            <td>5.33</td>
            <td>TBD</td>
            <td>TBD</td>
        </tr>
        <tr>
            <td rowspan="3">Recovery</td>
            <td>Low</td>
            <td>-3.07</td>
            <td>4.67</td>
            <td>-0.60</td>
            <td>1.78</td>
            <td>2.66</td>
            <td>1.95</td>
            <td>4.93</td>
            <td>TBD</td>
            <td>TBD</td>
        </tr>
        <tr>
            <td>Medium</td>
            <td>-3.19</td>
            <td>4.97</td>
            <td>-0.79</td>
            <td>1.65</td>
            <td>3.62</td>
            <td>3.15</td>
            <td>5.26</td>
            <td>TBD</td>
            <td>TBD</td>
        </tr>
        <tr>
            <td>High</td>
            <td>-8.19</td>
            <td>4.11</td>
            <td>-6.51</td>
            <td>-0.82</td>
            <td>3.52</td>
            <td>3.52</td>
            <td>4.84</td>
            <td>TBD</td>
            <td>TBD</td>
        </tr>
        <tr>
            <td rowspan="2">Skin Type</td>
            <td>Biking</td>
            <td>-6.39</td>
            <td>-3.38</td>
            <td>-4.21</td>
            <td>-5.50</td>
            <td>0.68</td>
            <td>0.57</td>
            <td>-0.28</td>
            <td>TBD</td>
            <td>TBD</td>
        </tr>
        <tr>
            <td>Stepping</td>
            <td>-12.59</td>
            <td>-9.06</td>
            <td>-11.41</td>
            <td>-12.51</td>
            <td>-3.13</td>
            <td>-2.85</td>
            <td>-4.50</td>
            <td>TBD</td>
            <td>TBD</td>
        </tr>
        <tr>
            <td>Overall</td>
            <td>Average</td>
            <td>-1.90</td>
            <td>3.67</td>
            <td>0.05</td>
            <td>1.92</td>
            <td>3.86</td>
            <td>3.56</td>
            <td>4.93</td>
            <td>TBD</td>
            <td>TBD</td>
        </tr>
    </table>
<h4>Data Sets</h4>
<a href="https://mahnob-db.eu/hci-tagging/">HCI tagging database</a>: This database is videos and images of 30 test subjects' biometric reactions to stimuli. This dataset includes images and biometric data for these subjects.<br>
<a href="https://osf.io/fdrbh/wiki/home/">OSF rPPG</a>: This dataset covers provide RGB images and videos that are tagged with the foreground and background of the image as well as the biometrics of the people in the image.<br>
 <a href="https://www.idiap.ch/dataset/cohface">COHFACE dataset</a>: This dataset consists of 160 minutes of 40 individuals of varying genders with tagged biometric data. The only downside to this database is that we will need assistance gaining access.<br>
Another option for data is to build our own test cases with images generated via mobile phone and heart rates captured the same way.<br>
    
<h4>Experimental Methodology</h4>
     Our testing approach will consist of two phases of testing. The first phase will consist of preliminary tests that test the initial operating capability of our project. Phase one will test the algorithm on data of people of the same skin tone, gender, with no facial hair, under ideal lighting conditions, with the test subject facing the camera straight on from a fixed distance. This methodology of testing will allow students to determine the algorithms operating capacity before other variables are introduced. The second phase of testing will introduce the variables that were fixed in the first phase. Each variable will be changed independently of the rest so to isolate that particular variable. This will enable students to  evaluate the algorithms performance in specific test cases.<br><br>
    
    This methodology for testing the algorithm allows for complete testing of variables while isolating the faults. The algorithm will be successful if the algorithm can achieve ±5% in phase one. Success is more difficult to define for the second phase of testing. Success in the second phase will be rated on initial performance versus improvement. If a test case in phase two has ±10% error then it is considered a success. If the error is greater than ±10% the success criteria will be defined by the amount of improvement that can be achieved due to tweaking the algorithm. 
    
<h4>Test Cases</h4>
    We will perform an experiment measuring the effect of subject distance from the camera on the accuracy of the derived PPG signal, compared to a reference measurement from a pulse oximeter sensor. We believe that distance may influence accuracy, due to decreasing pixel density of the face-image as the subject moves further from the camera. To test this, we will record 1 minute videos of the subject 0.5 meters from the camera, 1 meter from the camera, and 2 meters from the camera. Variables such as camera location, lighting, and video quality will be held constant. We expect that the derived PPG signals of further away subjects will have a higher signal-to-noise ratio, and thus be less accurate.<br><br>

We will also perform an experiment with the attempt to measure the program's ability to track a single face within a video, regardless of motion, both linear and rotational. This could potentially be one of the most challenging tests the program undergoes, as the heart rate is measured from very minor movements of the face, so moving the face throughout the video could drastically affect the results. As with the experiment listed above, these tests will be performed by recording 1 minute videos of the subject while they move their head at varying rates, the rest of the video, lighting and quality, will remain constant. We expect to see a decrease in the accuracy of the program as the basis of tracking heart rate is based on very precise motion. <br><br>
    An experiment will be performed on the image quality that is necessary to get valid data. One of the main flaws of image processing is the general lack of resistance to noisy data. One of the tests will add noise to the testing images and compare the accuracy. The other image quality test will be image resolution. This will involve downsampling the testing images to different degrees and taking metrics at each step. The expected performance in both of these situations will be decreased accuracy. The goal is to determine how robust the algorithm is. This will be achieved by incrementally testing by adding more noise/downsampling and comparing the results.

<br><br>

<h3>Progress Report 1 11/01/2020</h3>
<h4>Facial Recognition</h4>
    The facial recognition software for the project is at a good place. Currently, the software takes the input of a xml data from teh HCI Tagging Dataset. This data is then parsed for faces using Open CV. Then a bounding box is drawn around the center of the face to be used in th
<h4>Data Pipeline</h4>
<h4>Signal Filtering</h4>

<!-- Results -->
<h3>Qualitative results</h3>
These results will be changed upon completion of the project, but show some of the progress made so far.<br>
First, the facial detection via Haar Cascades can be seen here. With the RBG data overlaid onto the image.<br>
    <img style="height: 200px;" alt="" src="face_capture.png">
<br><br>
When we plot that data raw we get the following<br>
    <img style="height: 200px;" alt="" src="raw_data.png"><br>
This data on it's own is almost useless, which is why the need for data processing comes in. That data processing is where we are now, trying to convert our captured data into useable information about the patients vitals.
    
<br><br>
<!--Then this same data on the channel with the strongest power spectrum peak after being processed for interbeat interval calulation -->
    <!-- run the above data through the filters and pray the output looks nice -->
<br><br>

<br><br>

<h3>Citations</h3>
1. Poh, M.-Z., McDuff, D.J., Picard, R.W.: Advancements in noncontact, multiparameter physiological measurements using a webcam. IEEE Trans. Biomed. Eng. 58, 7–11 (2011) <br>
2. Zhan, Q. et al. “Analysis of CNN-based remote-PPG to understand limitations and sensitivities.” Biomedical optics express 11 3 (2020): 1268-1283.<br>




  <hr>
  <footer> 
  <p>© David Haas, Spencer Mullinix, Hogan Pope</p>
  </footer>
</div>
</div>

<br><br>

</body></html>